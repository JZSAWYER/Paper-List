# Paper-List

## Table of Contents

- [1. Large Language Models](#1-large-language-models)
    - [Factual Editing](#factual-editing)
        - [Summary](#summary)
        - [Memory-based](#memory-based)
        - [Locate-Then-Edit](#locate-then-edit)
        - [Meta-Learning](#meta-learning)
        - [Sequential Editing](#sequential-editing)
    - [Multimodel Editing](#multimodel-editing)
        - [MLLMs](#mllms)
        - [Benchmark](#benchmark)
    - [Safety](#safety)
    - [Alignment](#alignment)
    - [Reasoning](#reasoning)
    - [Emergent Ability](#emergent-ability)
    - [Theory](#theory)
- [2. Machine Learning](#2-machine-learning)
    - [Theory](#theory-1)
    - [Generative Models](#generative-models)
        - [Diffusion Models](#diffusion-models)
    - [Interpretability](#interpretability)
- [3. Pattern Recognition](#3-pattern-recognition)
    - [Computer Vision](#computer-vision)
- [4. Others](#4-others)

## 1. Large Language Models

### Factual Editing

#### Summary

- [2023/05] **Editing Large Language Models: Problems, Methods, and Opportunities.** *Yunzhi Yao, Peng Wang et al. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2305.13172)]
  - ..

- [2023/08] **EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models.** *Peng Wang et al. EMNLP 2023.* [[paper](https://doi.org/10.48550/arXiv.2308.07269)]
  - ..

#### Memory-based

- [2023/01] **Transformer-Patcher: One Mistake worth One Neuron.** *Zeyu Huang et al. ICLR 2023.* [[paper](https://doi.org/10.48550/arXiv.2301.09785)]
  - ..

- [2022/06] **Fast Model Editing at Scale** *Ce Zheng et al. ICLR 2022.* [[paper](https://doi.org/10.48550/arXiv.2308.07269)]
  - ..

#### Locate-Then-Edit

- [2022/03] **Knowledge Neurons in Pretrained Transformers.** *Damai Dai et al. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2104.08696)]
  - ..

- [2023/01] **Locating and Editing Factual Associations in GPT.** *Kevin Meng et al. NeurIPS 2022.* [[paper](https://doi.org/10.48550/arXiv.2202.05262)]
  - ..

- [2023/08] **Mass-Editing Memory in a Transformer.** *Kevin Meng et al. ICLR 2023.* [[paper](https://doi.org/10.48550/arXiv.2210.07229)]
  - ..

#### Meta-Learning

- [2023/03] **Can We Edit Factual Knowledge by In-Context Learning?** *Eric Mitchell et al. ICLR 2023.* [[paper](https://doi.org/10.48550/arXiv.2305.12740)]
  - ..

#### Sequential Editing

- [2023/01] **Transformer-Patcher: One Mistake worth One Neuron.** *Zeyu Huang et al. ICLR 2023.* [[paper](https://doi.org/10.48550/arXiv.2301.09785)]
  - T-Patcher

- [2023/10] **Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors.** *Thomas Hartvigsen et al. NeurIPS 2023.* [[paper](https://doi.org/10.48550/arXiv.2211.11031)]
  - GRACE

### Multimodel Editing

#### MLLMs

- [2023/10] **MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.** *Deyao Zhu and Jun Chen et al. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2304.10592)]
  - Target Model in MLLM Editing.

- [2023/06] **BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.** *Junnan Li et al. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2301.12597)]
  - Target Model in MLLM Editing.

#### Benchmark

- [2023/10] **Can We Edit Multimodal Large Language Models?** *Siyuan Cheng, Bozhong Tian et al. EMNLP 2023.* [[paper](https://doi.org/10.48550/arXiv.2310.08475)]
  - Benchmark for MLLM Editing.

- [2016/10] **VQA: Visual Question Answering.** *Aishwarya Agrawal et al. ICCV 2015.* [[paper](https://doi.org/10.48550/arXiv.1505.00468)]
  - VQA Dataset.

### Safety

- [2023/07] **FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios.** *I-Chun Chern et al. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2307.13528)]
  - ..

- [2023/05] **A Pretrainerâ€™s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity.** *Shayne Longpre et al. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2305.19187)]
  - ..

- [2023/08] **Whose Opinions Do Language Models Reflect?** *Shibani Santurkar et al. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2305.13172)]
  - ..

- [2023/07] **GPT detectors are biased against non-native English writers** *Weixin Liang and Mert Yuksekgonul and Yining Mao and Eric Wu and James Zou. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2304.02819)]
  - GPT detectors misclassify non-native English writing samples as AI-generated while not for native writing samples. Prompting GPT to generate more linguistically diverse versions of the non-native samples removes this bias.

### Alignment



### Reasoning

- [2023/08] **Cumulative Reasoning with Large Language Models** *Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao arXiv.* [[paper](https://doi.org/10.48550/arXiv.2308.04371)]
  - ..

- [2023/01] **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models** *Jason Wei et al. NeurIPS 2022.* [[paper](https://doi.org/10.48550/arXiv.2201.11903)]
  - CoT



### Emergent Ability

- [2023/07] **Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study.** *Peiyu Liu et al. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2307.08072)]
  - ..

### Theory

- [2023/06] **On the Power of Foundation Models.** *Yang Yuan. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2211.16327)]
  - ..

- [2023/03] **Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers.** *Damai Dai et al. ACL 2023.* [[paper](https://doi.org/10.48550/arXiv.2212.10559)]
  - ..


## 2. Machine Learning


### Theory

- [2023/09] **Transformers as Support Vector Machines. EMNLP 2023.** *Davoud Ataee Tarzanagh et al. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2308.16898)]
  - ..

- [2023/03] **Succinct Representations for Concepts.** *Yang Yuan. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2303.00446)]
  - ..

- [2023/08] **A Categorical Framework of General Intelligence.** *Yang Yuan. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2303.04571)]
  - ..

- [2020/11] **Every Model Learned by Gradient Descent Is Approximately a Kernel Machine.** *Pedro Domingos. Deep Learning Reviews.* [[paper](https://doi.org/10.48550/arXiv.2012.00152)]
  - ..

- [2020/02] **Neural Tangent Kernel: Convergence and Generalization in Neural Networks.** *Arthur Jacot et al. arXiv.* [[paper](https://doi.org/10.48550/arXiv.1806.07572)] 
  - ..

- [2020/06] **Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains.** *Matthew Tancik, Pratul P. Srinivasan and Ben Mildenhall et al. NeurIPS 2020.* [[paper](https://doi.org/10.48550/arXiv.2006.10739)] 
  - ..

- [2019/08] **Adversarial Examples Are Not Bugs, They Are Features.** *Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras and Logan Engstrom et al. NeurIPS 2019.* [[paper](https://doi.org/10.48550/arXiv.1905.02175)] 
  - ..

- [2022/06] **The Dimpled Manifold Model of Adversarial Examples in Machine Learning.** *Adi Shamir et al. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2106.10151)] 
  - ..

### Generative models

#### Diffusion Models

- [2023/10] **Diffusion Models: A Comprehensive Survey of Methods and Applications.** *Ling Yang and Zhilong Zhang et al. ACM Computing Surveys, 2022.* [[paper](https://doi.org/10.48550/arXiv.2209.00796)] 
  -  ..
  
- [2015/11] **Deep unsupervised learning using nonequilibrium thermodynamics.** *Sohl-Dickstein et al. ICML 2015.* [[paper](https://doi.org/10.48550/arXiv.1503.03585)] 
  - The first paper on diffusion models.

- [2020/12] **Denoising Diffusion Probabilistic Models.** *Jonathan Ho et al. NeurIPS 2020.* [[paper](https://doi.org/10.48550/arXiv.2006.11239)] 
  - DDPM, with FID score of 3.17 on CIFAR10.

- [2021/02] **Improved Denoising Diffusion Probabilistic Models.** *Alex Nichol and Prafulla Dhariwal. ICML 2021.* [[paper](https://doi.org/10.48550/arXiv.2102.096729)] 
  - Improved DDPM, with FID score of 2.94 on CIFAR10, 12.3 on ImageNet 256*256.

- [2021/06] **Diffusion Models Beat GANs on Image Synthesis.** *Prafulla Dhariwal and Alex Nichol. NeurIPS 2021.* [[paper](https://doi.org/10.48550/arXiv.2105.05233)] 
  - OpenAI introduced ADM-G, with FID score of 4.59 on ImageNet 256*256, huge improvement.

- [2021/02] **Score-based Generative Modeling through Stochastic Differential Equations.** *Yang Song et al.  ICLR 2021 (Oral).* [[paper](https://doi.org/10.48550/arXiv.2011.13456)]  
  - ..

- [2022/04] **High-Resolution Image Synthesis with Latent Diffusion Models.** *Robin Rombach and Andreas Blattmann et al.  CVPR 2022 (Oral).* [[paper](https://doi.org/10.48550/arXiv.2112.10752)]  
  - Stable Diffusion.

### Interpretability

- [2021/09] **Transformer Feed-Forward Layers Are Key-Value Memories.** *Mor Geva et al. EMNLP 2021.* [[paper](https://doi.org/10.48550/arXiv.2012.14913)]
  - ..

## 3. Pattern Recognition

### Computer Vision

- [2021/09] **An Unsupervised Deep Learning Approach for Real-World Image Denoising.** *Dihan Zheng et al. ICLR 2021.* [[paper](https://openreview.net/forum?id=tIjRAiFmU3y)]
  - ..

- [2022/09] **Learn from Unpaired Data for Image Restoration: A Variational Bayes Approach.** *Dihan Zheng et al. TPAMI 2022.* [[paper](https://doi.org/10.48550/arXiv.2012.14913)]
  - ..

## 4. Others

- [2023/03] **GPT-4 Technical Report.** *OpenAI. arXiv.* [[paper](https://doi.org/10.48550/arXiv.2303.08774)]
  - Full analysis of GPT-4.

